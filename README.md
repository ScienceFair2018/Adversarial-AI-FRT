# Adversarial AI: Bypassing Facial Recognition

This project explores the vulnerability of facial recognition systems to adversarial attacks, with a focus on Fast Gradient Sign Method (FGSM), Projected Gradient Descent (PGD), and random noise perturbations. The experiments analyze how small, targeted modifications to facial images can significantly impact identity embeddings generated by the FaceNet model.

## üìÇ Repository Contents

### üìÅ Notebooks
- `Adversarial_AI_code.ipynb`: Core Grad-CAM-based implementation of FGSM and PGD attacks.
- `celebrity_attack_evaluation.ipynb`: Evaluation of attacks over 2,000+ images from the LFW dataset using cosine similarity metrics.
  
### üñºÔ∏è Sample Images
Images of public figures and sample perturbation results:
- `joe_biden.jpeg`, `biden1.jpg`, `biden2.jpg`, `biden edited.jpg`
- `valentina 2.jpg`, `chadwick 2.jpg`, `hemsworth.jpg`, `will smith.jpg`, `pratt.jpg`
- Augmented versions: `bronzer.jpg`, `lipstick.jpg`, `scars.jpg`, `arctic.jpg`

These are used to qualitatively visualize the perceptual effects of adversarial noise.

## üöÄ Usage

To run the experiments:

1. Clone the repository and open either notebook in Jupyter or Google Colab.
2. Install dependencies:
   ```bash
   pip install facenet-pytorch scipy matplotlib tqdm
   ```
3. Run `Adversarial_AI_code.ipynb` to generate and visualize Grad-CAM-guided adversarial examples.
4. Run `celebrity_attack_evaluation.ipynb` to compute cosine similarity results across a larger dataset. The dataset should automatically mount if using google collab.

## üß† Abstract
Facial recognition technologies (FRT) are increasingly used by authoritarian regimes as instruments of surveillance and control, exemplified by the targeted monitoring and persecution of Uyghur individuals in China. Prior research has shown that subtle, global image perturbations‚Äîsuch as those generated by the Fast Gradient Sign Method (FGSM) and Projected Gradient Descent (PGD)‚Äîcan mislead FRT systems without altering the natural appearance of faces. I applied these techniques to a large-scale evaluation on over 2,000 facial images from the Labeled Faces in the Wild (LFW) dataset. The results demonstrate that PGD reduces identity similarity scores by over 50\% at moderate perturbation levels ($\epsilon = 0.05$), significantly outperforming FGSM and random noise. However, these digital attacks are often impractical in real-world settings where physical implementation is constrained. To address this gap, I adapted the Grad-CAM algorithm to localize facial regions most salient to FRT models, enabling targeted, interpretable perturbations. Because facial recognition models rely on different features for different individuals, this approach supports personalized adversarial strategies. A pilot experiment further demonstrated the effectiveness of applying makeup to these critical regions as a form of physical adversarial intervention. This research underscores the role of adversarial machine learning in advancing public discourse on AI surveillance, digital privacy, and the protection of vulnerable populations and dissidents.
